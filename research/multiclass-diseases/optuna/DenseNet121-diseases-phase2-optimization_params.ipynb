{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8986e864-9f87-43a7-9d43-dafc227f3b67",
   "metadata": {},
   "source": [
    "# Lung Diseases Classification - Phase 2: Optimization Hyperparameters Search with Optuna\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../../images/optuna_logo.jpg\" alt=\"Crop with image and mask\" style=\"display: block; margin: 0 auto;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "## Context\n",
    "This notebook represents the **second phase** of the lung disease classification pipeline, focusing on **optimization-level hyperparameter tuning** after the model architecture has already been fixed.\n",
    "\n",
    "In the previous phase, Optuna was used to search for the **best classifier head architecture** on top of a frozen DenseNet121 backbone, including:\n",
    "- Number of dense layers in the classification head  \n",
    "- Number of units per dense layer  \n",
    "\n",
    "With the architecture now finalized, this notebook shifts attention to **training dynamics and regularization**, which play a critical role in model stability, convergence behavior, and generalization performance—especially in medical imaging tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "The main goal of this notebook is to identify the **optimal optimization hyperparameters** for a multi-class lung disease classifier that predicts:\n",
    "- **COVID-19**\n",
    "- **Viral Pneumonia**\n",
    "- **Lung Opacity**\n",
    "\n",
    "using ROI-focused chest X-ray images processed through a DenseNet121 backbone.\n",
    "\n",
    "The optimization process is driven by **Optuna**, guided by a **custom penalized F1-score objective** that balances predictive performance with training stability.\n",
    "\n",
    "\n",
    "## Key Optimization Parameters Explored\n",
    "\n",
    "This phase focuses on tuning **training dynamics and regularization hyperparameters** that directly affect convergence behavior, stability, and generalization performance. The following parameters are explored using Optuna:\n",
    "\n",
    "- **Stage-wise Learning Rates**\n",
    "  - `lr_stage1`: Learning rate used during the initial frozen-backbone training stage  \n",
    "  - `lr_stage2`: Lower learning rate applied during fine-tuning for stable convergence  \n",
    "\n",
    "- **Weight Decay**\n",
    "  - Controls L2 regularization strength to reduce overfitting and improve generalization  \n",
    "\n",
    "- **Dropout Rate**\n",
    "  - Applied to classifier head layers to regularize dense representations and prevent co-adaptation  \n",
    "\n",
    "- **Label Smoothing**\n",
    "  - Introduces controlled uncertainty in ground-truth labels to improve calibration and robustness  \n",
    "\n",
    "- **Class Weight Power (`weight_power`)**\n",
    "  - A tunable exponent applied to class weights  \n",
    "  - `0.0` enforces equal weighting across classes  \n",
    "  - `1.0` applies raw class imbalance weights  \n",
    "  - Intermediate values allow smooth interpolation between these extremes  \n",
    "\n",
    "All optimization parameters are evaluated jointly under a consistent training protocol, ensuring that improvements are driven by learning dynamics rather than architectural changes.\n",
    "\n",
    "\n",
    "## Training & Evaluation Strategy\n",
    "\n",
    "- **Backbone**: ImageNet-pretrained DenseNet121  \n",
    "- **Input Focus**: Lung ROI cropping based on segmentation masks (no hard masking)  \n",
    "- **Data Pipeline**:\n",
    "  - TFRecord-based datasets\n",
    "  - Class remapping and one-hot encoding\n",
    "  - Optional augmentation applied only during training\n",
    "- **Metrics**:\n",
    "  - Accuracy\n",
    "  - Precision\n",
    "  - Recall\n",
    "  - Macro F1-score\n",
    "  - AUC\n",
    "\n",
    "A **custom penalized F1-score** is used as the Optuna objective, discouraging:\n",
    "- Large precision–recall gaps\n",
    "- Overfitting (via train–validation loss divergence)\n",
    "\n",
    "## Utilities & Modularity\n",
    "\n",
    "To maintain clarity and reproducibility, all reusable logic is abstracted into a dedicated `utils.py` file, including:\n",
    "- Dataset construction and preprocessing\n",
    "- Model creation and compilation\n",
    "- Reproducibility utilities\n",
    "- Memory cleanup between trials\n",
    "- Custom scoring functions\n",
    "\n",
    "This design keeps the notebook focused on **experiment orchestration**, while core logic remains centralized and reusable.\n",
    "\n",
    "## Outcome\n",
    "\n",
    "At the end of this notebook, the Optuna study produces:\n",
    "- The **best-performing optimization hyperparameters**\n",
    "- A statistically grounded configuration ready for:\n",
    "  - Final training\n",
    "  - Cross-validation\n",
    "  - Deployment or downstream disease-specific modeling stages\n",
    "\n",
    "This phase completes the systematic two-stage Optuna search strategy:\n",
    "1. **Architecture search**\n",
    "2. **Optimization hyperparameter search**\n",
    "\n",
    "forming a robust foundation for reliable lung disease classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a8e96c-d393-4f7c-8660-88dbd3c118ff",
   "metadata": {},
   "source": [
    "## Section 1 - Environment Setup & Experiment Initialization\n",
    "\n",
    "This section initializes the experimental environment required for the Optuna-based optimization process.  \n",
    "It establishes consistent dependencies, enforces reproducibility, configures GPU memory behavior, and selects the most suitable hardware execution strategy.\n",
    "\n",
    "These steps are essential to ensure that all optimization trials are conducted under **controlled, repeatable, and hardware-aware conditions**, allowing fair comparison across experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e2bfe8-803f-4295-8e00-f5ce720d09b2",
   "metadata": {},
   "source": [
    "### 1.1 Library Imports and Dependencies\n",
    "\n",
    "Imports all core libraries required for configuration management, numerical computation, model training, and hyperparameter optimization.  \n",
    "Reusable project-specific logic is centralized in a shared `utils.py` module, keeping the notebook focused on experiment orchestration rather than implementation details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a9a5343-10d9-482d-9ad0-90e3bb1aab26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-31 19:43:09.062228: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-31 19:43:09.611859: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-31 19:43:12.893469: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import optuna\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "\n",
    "# Import shared utilities for data pipelines, model construction,\n",
    "# training helpers, and experiment management\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c962a741-4eae-4271-abd1-7bc8ba5a9dca",
   "metadata": {},
   "source": [
    "### 1.2 Framework Version Logging\n",
    "\n",
    "Logging the TensorFlow/Keras version provides traceability and helps ensure consistency across experimental runs.  \n",
    "This is particularly important when comparing results across environments or revisiting experiments at a later stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f2cc78-de8b-4bea-bd81-472f6e344adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow / Keras Version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "# Log TensorFlow/Keras version for experiment reproducibility and debugging\n",
    "print(f\"TensorFlow / Keras Version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287a1214-45bf-4540-bc48-ff769c2abe1f",
   "metadata": {},
   "source": [
    "### 1.3 Reproducibility and Hardware Configuration\n",
    "\n",
    "Now enforce deterministic behavior by seeding all relevant random number generators.  \n",
    "It also enables safe GPU memory growth to avoid unnecessary memory allocation and automatically selects the optimal distribution strategy (TPU, multi-GPU, or CPU) based on the available hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "214f7b7c-252a-4a6a-b42f-7259e03ebf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For reproducibility, everything seeded!\n",
      "Enabled memory growth for 1 GPU(s)\n",
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n",
      "INFO:tensorflow:Initializing the TPU system: local\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Using GPU strategy: MirroredStrategy\n",
      "REPLICAS: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1768070208.911453   71042 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2246 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Global seed to ensure deterministic behavior across runs\n",
    "SEED = 28\n",
    "seed_everthing(SEED)\n",
    "\n",
    "# Enable safe GPU memory allocation (prevents full memory reservation)\n",
    "gpu_growth()\n",
    "\n",
    "# Automatically select the optimal distribution strategy\n",
    "# (TPU → Multi-GPU → CPU fallback)\n",
    "strategy = get_strategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9214594d-2908-4042-8f0a-c9cf67e0b438",
   "metadata": {},
   "source": [
    "## Section 2 – Global Configuration and Dataset Metadata\n",
    "\n",
    "Ok in this Section define all **global constants and configuration dictionaries** used throughout the optimization process.  \n",
    "Centralizing these parameters ensures **clarity**, **reproducibility**, and **consistent behavior** across Optuna trials.\n",
    "\n",
    "The configuration is split into logical groups covering:\n",
    "- Data pipeline behavior\n",
    "- Model and optimization settings\n",
    "- Training-scale parameters under distributed execution\n",
    "- Dataset label mappings\n",
    "\n",
    "### 2.1 Core System and Data Parameters\n",
    "\n",
    "Now we define fundamental constants that control dataset loading, input dimensions, and model structure.  \n",
    "These values remain fixed across all optimization trials, ensuring that Optuna explores **optimization dynamics only**, not structural variations.\n",
    "\n",
    "Key parameters such as image resolution, number of classes, and shuffle buffer size directly influence both **training stability** and **data throughput**.\n",
    "\n",
    "#### 2.1.1 Algorithm and Optimization Configuration\n",
    "\n",
    "Then groups all algorithm-level configuration dictionaries used during training and optimization.  \n",
    "Each dictionary encapsulates a specific aspect of the experimental protocol, making the setup both **explicit** and **easily adjustable**.\n",
    "\n",
    "Notably:\n",
    "- The penalized $F_1$ configuration controls how performance is evaluated across training stages.\n",
    "- Optimization parameters define the number of Optuna trials and the fine-tuning schedule.\n",
    "- Model configuration fixes the input and output interface of the network.\n",
    "\n",
    "#### 2.1.2 Distributed Training Batch Configuration\n",
    "\n",
    "This subsection defines batch-size scaling under distributed training.  \n",
    "The **global batch size** is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Global Batch Size} = \\text{Batch Size per Replica} \\times N_{\\text{replicas}}\n",
    "$$\n",
    "\n",
    "This formulation ensures that training behavior remains consistent when scaling across multiple devices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74ff4361-7d4c-4db9-a1c6-81d0e239933d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Batch size: 8\n"
     ]
    }
   ],
   "source": [
    "# Enable automatic tuning of tf.data parallelism\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "# Base directories for dataset files and saved models\n",
    "DATA_DIR = '../../data/tfrecords/'\n",
    "MODELS_DIR = '../../models/'\n",
    "# Input image resolution used throughout training\n",
    "IMAGE_SIZE = (256, 256)\n",
    "# Buffer size for dataset shuffling\n",
    "SHUFFLE_SIZE = 1024\n",
    "# Number of target disease classes (COVID, Viral Pneumonia, Lung Opacity)\n",
    "NUM_CLASSES = 3\n",
    "# Mask resolution is kept identical to image resolution\n",
    "MASK_SIZE = IMAGE_SIZE\n",
    "# Backbone layer name used as the starting point for fine-tuning\n",
    "UNFREEZE_LAYER = 'conv5_block1_0_bn'\n",
    "# === ALGORITHM CONFIGS ===\n",
    "# Configuration for the penalized F1-score objective\n",
    "# alpha_p controls penalty strength, stage defines evaluation window\n",
    "PENALIZED_F1_CONFIG = {'alpha_p': 0.5, 'stage': 3}\n",
    "# Optuna-driven optimization schedule and fine-tuning control\n",
    "OPTIMIZATION_CONFIG = {\n",
    "    'opt_trials': 30,              # Number of Optuna trials\n",
    "    'opt_warmup_epoch': 3,          # Frozen-backbone warm-up epochs\n",
    "    'opt_unfreeze_epoch': 5,        # Epoch to start fine-tuning\n",
    "    'unfreeze_layer': UNFREEZE_LAYER\n",
    "}\n",
    "# Fixed model interface configuration\n",
    "MODEL_CONFIG = {\n",
    "    'img_size': IMAGE_SIZE,\n",
    "    'mask_size': MASK_SIZE,\n",
    "    'num_classes': NUM_CLASSES\n",
    "}\n",
    "# Batch size processed by each replica (GPU/TPU core)\n",
    "BATCH_SIZE_PER_REPLICA = 8\n",
    "# Effective batch size across all synchronized replicas\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "# Dataset-related runtime parameters\n",
    "DATASET_CONFIG = {\n",
    "    'shuffle': SHUFFLE_SIZE,\n",
    "    'batch_size': GLOBAL_BATCH_SIZE,\n",
    "    'auto': AUTO\n",
    "}\n",
    "# Log final batch size for verification\n",
    "print(f'Global Batch size: {GLOBAL_BATCH_SIZE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383c104d-24f6-4389-a38f-75e06763c361",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.2 Dataset Class Mapping (Original Labels)\n",
    "\n",
    "Then we load the original class-to-index mapping used during dataset creation.  \n",
    "Preserving and explicitly logging this mapping is critical for **label integrity**, **debugging**, and **correct interpretation** of evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c85b4040-30e3-457c-9286-c3898bb8aaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'COVID': 0, 'Normal': 1, 'Viral Pneumonia': 2, 'Lung_Opacity': 3}\n"
     ]
    }
   ],
   "source": [
    "# Load original dataset class-to-index mapping\n",
    "class_mapping_path = '../../data/class_mapping.json'\n",
    "with open(class_mapping_path, 'r') as f:\n",
    "    class_mapping = json.load(f)\n",
    "\n",
    "# Display mapping for validation and traceability\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4dc710-8af4-455b-ac03-462959381989",
   "metadata": {},
   "source": [
    "### 2.5 Dataset Class Mapping (Filtered Unhealthy Classes)\n",
    "\n",
    "Now load the remapped label definitions used after filtering out healthy samples.  \n",
    "The resulting mapping aligns the dataset with the **three-class disease classification task**, ensuring consistent label semantics throughout training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25a98b8b-f899-4432-a264-3551b66608c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'COVID': 0, 'Viral Pneumonia': 1, 'Lung Opacity': 2}\n"
     ]
    }
   ],
   "source": [
    "# Load remapped labels after excluding healthy samples\n",
    "unhealthy_class_mapping_path = '../../data/unhealthy_mapping.json'\n",
    "with open(unhealthy_class_mapping_path, 'r') as f:\n",
    "    unhealthy_class_mapping = json.load(f)\n",
    "\n",
    "# Display final disease-class mapping\n",
    "print(unhealthy_class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81219e7-c1a5-402e-a2db-9ab714312575",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3 – Dataset Construction for Optuna Optimization\n",
    "\n",
    "A lightweight training and validation dataset is constructed to support efficient Optuna trials.  \n",
    "Rather than using the full dataset, a **restricted subset of TFRecord files** is intentionally selected to reduce computational cost while preserving representative data statistics.\n",
    "\n",
    "This setup enables rapid hyperparameter exploration without compromising the validity of the optimization process.\n",
    "\n",
    "### 3.1 TFRecord File Selection for Optimization\n",
    "\n",
    "A small subset of TFRecord files is extracted from the full dataset for Optuna-driven experimentation.  \n",
    "Using a reduced training split significantly accelerates trial execution while maintaining consistent preprocessing and label semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d95524c-fb43-41cb-97fe-403ca75f842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect and sort all available TFRecord files\n",
    "all_files = sorted(tf.io.gfile.glob(os.path.join(DATA_DIR, '*.tfrecord')))\n",
    "# Subset selection for Optuna trials (lightweight training set)\n",
    "sub_train_files = all_files[:3]\n",
    "# Validation split reserved from the dataset tail\n",
    "val_files = all_files[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a266b0-8dfa-4294-a9b9-9a819b2c3e5e",
   "metadata": {},
   "source": [
    "### 3.2 Parsing, Label Remapping, and Preprocessing Configuration\n",
    "\n",
    "Parsing logic, label remapping, ROI-based preprocessing, and backbone-specific input normalization are assembled into a unified dataset configuration.  \n",
    "This ensures that all Optuna trials operate on **identical input transformations**, isolating optimization effects from data inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4447f33-61fc-42ce-8127-6c05eb805a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFRecord parsing function with fixed image and mask resolution\n",
    "parse_fn = make_parse_fn(config=MODEL_CONFIG)\n",
    "# Remap labels to a contiguous multi-class space after filtering\n",
    "remap_for_multiclass = make_remap_for_multiclass(NUM_CLASSES)\n",
    "# Attach dataset transformation components to the shared config\n",
    "DATASET_CONFIG['parse_fn'] = parse_fn\n",
    "DATASET_CONFIG['remap'] = remap_for_multiclass\n",
    "DATASET_CONFIG['roi'] = lung_roi_preprocess\n",
    "DATASET_CONFIG['preprocess'] = preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a217bb5-009b-474e-a925-f8d87a46dd85",
   "metadata": {},
   "source": [
    "### 3.3 Lightweight Data Augmentation Pipeline\n",
    "\n",
    "A restrained augmentation pipeline is used during optimization to introduce mild spatial and intensity variability.  \n",
    "Augmentations are intentionally conservative to stabilize Optuna evaluations while still improving robustness to minor appearance changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ad7ecc7-96ed-4718-a6a2-bbf45a9ea6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a1mohamad/ai-env/lib/python3.12/site-packages/keras/src/layers/preprocessing/tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Sequential augmentation pipeline applied at the batch level\n",
    "light_augmentation = tf.keras.Sequential([\n",
    "    # Geometric transformations\n",
    "    tfl.RandomFlip('horizontal', input_shape= IMAGE_SIZE + (3,)),\n",
    "    tfl.RandomRotation(0.05, interpolation='bilinear', fill_mode='nearest'),\n",
    "    tfl.RandomZoom(0.05, interpolation='bilinear', fill_mode='nearest'),\n",
    "    \n",
    "    # Intensity and contrast adjustments\n",
    "    tfl.RandomContrast(0.05),\n",
    "    tfl.RandomBrightness(0.05)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e901f9b-9e2f-4a17-8060-b5fa9341d311",
   "metadata": {},
   "source": [
    "### 3.4 Training and Validation Dataset Assembly\n",
    "\n",
    "Training and validation datasets are instantiated using a shared construction pipeline.  \n",
    "Augmentation is applied **only to the training split**, while validation data remains deterministic for reliable metric comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d018bf1-29f6-416d-ba80-1e3d70c3c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training dataset for Optuna trials\n",
    "sub_train_dataset = multiclass_dataset(\n",
    "    sub_train_files,\n",
    "    config=DATASET_CONFIG,\n",
    "    is_training=True,\n",
    "    image_augmentation=light_augmentation\n",
    ")\n",
    "\n",
    "# Build validation dataset without stochastic augmentation\n",
    "val_dataset = multiclass_dataset(\n",
    "    val_files,\n",
    "    config=DATASET_CONFIG,\n",
    "    is_training=False,\n",
    "    image_augmentation=light_augmentation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292e04ae-470a-47dc-8585-c867840f5ee5",
   "metadata": {},
   "source": [
    "### 3.5 Dataset Metadata and Optimization Parameters\n",
    "\n",
    "Dataset statistics are computed in a single pass to derive:\n",
    "- Effective training and validation steps\n",
    "- Class weights for imbalanced learning\n",
    "- Optuna-specific runtime parameters\n",
    "\n",
    "These values are injected directly into the optimization configuration to keep trial logic compact and consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12f141ad-5994-43c4-b3da-ae45bba02ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-10 22:07:01.202135: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:390] TFRecordDataset `buffer_size` is unspecified, default to 262144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Training Steps: 411\n",
      "Validation Steps: 131\n",
      "Class weights: [1.0320151  2.781726   0.59825325]\n"
     ]
    }
   ],
   "source": [
    "# Extract dataset statistics required for optimization\n",
    "optuna_metadata = get_dataset_metadata(sub_train_dataset, GLOBAL_BATCH_SIZE)\n",
    "# Number of training steps per Optuna trial\n",
    "optuna_steps = optuna_metadata[\"steps\"]\n",
    "# Class weights derived from dataset distribution\n",
    "weights = optuna_metadata[\"weights\"]\n",
    "weights = np.array([weights[k] for k in weights.keys()])\n",
    "# Validation step count\n",
    "validation_steps = count_steps_from_dataset(val_dataset)\n",
    "# Log derived metadata for verification\n",
    "print(f\"Optuna Training Steps: {optuna_steps}\\nValidation Steps: {validation_steps}\")\n",
    "print(f\"Class weights: {weights}\")\n",
    "# Inject dataset-dependent parameters into optimization config\n",
    "OPTIMIZATION_CONFIG['optuna_steps'] = int(optuna_steps)\n",
    "OPTIMIZATION_CONFIG['validation_steps'] = int(validation_steps)\n",
    "OPTIMIZATION_CONFIG['class_weights'] = weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3289cf-f285-4c88-aeef-ad99b2a90c3a",
   "metadata": {},
   "source": [
    "## Section 4 – Optuna Optimization Objective and Study Execution\n",
    "\n",
    "This section defines the core **Optuna optimization logic** used to identify the best training and regularization hyperparameters for the disease classification model.\n",
    "\n",
    "The classifier architecture is **fixed and inherited from Phase 1**, while Optuna explores optimization-related parameters such as learning rates, regularization strength, dropout, and class-weight scaling.  \n",
    "Model performance is evaluated using a **custom penalized $F_1$ score**, designed to balance accuracy, calibration, and generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f11cc84-5592-41fc-9963-331c51b9b47c",
   "metadata": {},
   "source": [
    "### 4.1 Loading Best Architecture from Phase 1\n",
    "\n",
    "Architecture metadata obtained from the Phase 1 Optuna search is loaded and reused without modification.  \n",
    "This guarantees a clean separation between **architecture search** and **optimization search**, ensuring that improvements are attributable solely to training dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ed63459-af3c-41f4-8925-9eeb5f9188ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Architecture metadata: {'best_trial_number': 16, 'best_value': 0.924062991142273, 'best_hparams': {'num_layers': 2, 'dense_units': [256, 64]}, 'phase1_settings': {'loss': 'CategoricalCrossentropy', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'dropout_rate': 0.1}, 'timestamp': 1767628988.7223353, 'seed': 28}\n",
      "Best Architecture Hyperparameters:\n",
      "Number of Dense layers: 2\n",
      "Dense units in order: [256, 64]\n"
     ]
    }
   ],
   "source": [
    "# Load best architecture metadata obtained from Phase 1\n",
    "best_arch_dir = './phase1_architecture/best_architecture.json'\n",
    "with open(best_arch_dir, 'r') as f:\n",
    "    best_arch = json.load(f)\n",
    "# Log selected architecture details for traceability\n",
    "print(f\"Best Architecture metadata: {best_arch}\")\n",
    "print(\n",
    "    f\"Best Architecture Hyperparameters:\\n\"\n",
    "    f\"Number of Dense layers: {best_arch['best_hparams']['num_layers']}\\n\"\n",
    "    f\"Dense units in order: {best_arch['best_hparams']['dense_units']}\"\n",
    ")\n",
    "# Attach Phase 1 hyperparameters to optimization config\n",
    "OPTIMIZATION_CONFIG['phase1_hparams'] = best_arch['best_hparams']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d60c7b-bc39-423c-a2e5-59377c17bb06",
   "metadata": {},
   "source": [
    "### 4.2 Optimization Hyperparameter Search Space Definition\n",
    "\n",
    "The Optuna search space is defined for **optimization-specific hyperparameters** while preserving the Phase 1 architecture.  \n",
    "These parameters control learning dynamics, regularization strength, and class imbalance handling during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dec6d62c-768c-4430-9ad7-14132ea7644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization_hparams(trial, hparams=best_arch[\"best_hparams\"]):\n",
    "    \"\"\"\n",
    "    Define the Optuna search space for optimization-related hyperparameters.\n",
    "\n",
    "    Architecture parameters are reused from Phase 1, while learning rates,\n",
    "    regularization, and class-weight scaling are tuned in this phase.\n",
    "    \"\"\"\n",
    "    num_layers = hparams[\"num_layers\"]\n",
    "\n",
    "    # Stage-wise learning rates\n",
    "    hparams[\"lr_stage1\"] = trial.suggest_float(\"lr_stage1\", 1e-4, 1e-3, log=True)\n",
    "    hparams[\"lr_stage2\"] = trial.suggest_float(\"lr_stage2\", 5e-6, 5e-5, log=True)\n",
    "\n",
    "    # Regularization parameters\n",
    "    hparams[\"weight_decay\"] = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    hparams[\"label_smoothing\"] = trial.suggest_categorical(\n",
    "        \"label_smoothing\", [0.0, 0.025, 0.05, 0.075, 0.1]\n",
    "    )\n",
    "\n",
    "    # Dropout applied to classifier head\n",
    "    hparams[\"dropout_rate\"] = trial.suggest_float(\"dropout_rate\", 0.1, 0.5, step=0.05)\n",
    "\n",
    "    # Power applied to class weights (0 → uniform, 1 → raw imbalance)\n",
    "    hparams[\"weight_power\"] = trial.suggest_float(\"weight_power\", 0.0, 1.0)\n",
    "    \n",
    "    return hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd30ebe-f995-4265-a4ff-ea38714ad1a4",
   "metadata": {},
   "source": [
    "### 4.3 Optuna Objective Function with Two-Stage Training\n",
    "\n",
    "The objective function defines a **two-stage training protocol**:\n",
    "\n",
    "1. **Warm-up stage** with a frozen backbone  \n",
    "2. **Fine-tuning stage** with selective backbone unfreezing and cosine learning-rate decay  \n",
    "\n",
    "Each trial is evaluated using a penalized $F_1$ score that accounts for:\n",
    "- Precision–recall imbalance\n",
    "- Overfitting via loss divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d241475-bfa8-47fe-8b16-caeba9664921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_optimization(trial, config=OPTIMIZATION_CONFIG):\n",
    "    \"\"\"\n",
    "    Optuna objective function implementing a two-stage training strategy\n",
    "    with frozen and unfrozen backbone phases.\n",
    "    \"\"\"\n",
    "    print('=' * 180)\n",
    "    print(f\"Trial {trial.number + 1}/{config['opt_trials']} started...\")\n",
    "\n",
    "    model = None\n",
    "    history = None\n",
    "    callbacks = None\n",
    "\n",
    "    # Training schedule parameters\n",
    "    warmup_epoch = config['opt_warmup_epoch']\n",
    "    unfreeze_epoch = config[\"opt_unfreeze_epoch\"]\n",
    "    total_epoch = warmup_epoch + unfreeze_epoch\n",
    "\n",
    "    # Dataset and optimization metadata\n",
    "    optuna_steps = config['optuna_steps']\n",
    "    validation_steps = config['validation_steps']\n",
    "    raw_class_weights = config['class_weights']\n",
    "    arch_hparams = config['phase1_hparams']\n",
    "    unfreeze_layer = config[\"unfreeze_layer\"]\n",
    "\n",
    "    # Sample optimization hyperparameters\n",
    "    hparams = optimization_hparams(trial, hparams=arch_hparams)\n",
    "    trial.set_user_attr(\"hparams\", hparams)\n",
    "\n",
    "    dropout_rate = hparams[\"dropout_rate\"]\n",
    "    p = hparams[\"weight_power\"]\n",
    "\n",
    "    # Apply power scaling to class weights\n",
    "    class_weights = dict(enumerate(np.power(raw_class_weights, p)))\n",
    "\n",
    "    # Loss function with label smoothing\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "        label_smoothing=hparams['label_smoothing']\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # ---------------------------\n",
    "        # Stage 1: Frozen backbone\n",
    "        # ---------------------------\n",
    "        with strategy.scope():\n",
    "            optimizer = tf.keras.optimizers.AdamW(\n",
    "                learning_rate=hparams[\"lr_stage1\"],\n",
    "                weight_decay=hparams[\"weight_decay\"]\n",
    "            )\n",
    "\n",
    "            model = densenet_model(\n",
    "                hparams,\n",
    "                dropout_rate=dropout_rate,\n",
    "                config=MODEL_CONFIG,\n",
    "                phase='opt'\n",
    "            )\n",
    "            model = compile_model(model, loss=loss, optimizer=optimizer)\n",
    "\n",
    "        history = model.fit(\n",
    "            sub_train_dataset.repeat(),\n",
    "            validation_data=val_dataset.repeat(),\n",
    "            epochs=warmup_epoch,\n",
    "            steps_per_epoch=optuna_steps,\n",
    "            validation_steps=validation_steps,\n",
    "            class_weight=class_weights\n",
    "        )\n",
    "\n",
    "        # ---------------------------\n",
    "        # Stage 2: Fine-tuning\n",
    "        # ---------------------------\n",
    "        with strategy.scope():\n",
    "            model = unfreeze_backbone(\n",
    "                model,\n",
    "                backbone_name='densenet',\n",
    "                unfreeze_layer=unfreeze_layer\n",
    "            )\n",
    "\n",
    "            decay_steps = total_epoch * optuna_steps\n",
    "            lr = tf.keras.optimizers.schedules.CosineDecay(\n",
    "                decay_steps=decay_steps,\n",
    "                initial_learning_rate=hparams[\"lr_stage2\"],\n",
    "                alpha=0.0\n",
    "            )\n",
    "\n",
    "            optimizer = tf.keras.optimizers.AdamW(\n",
    "                learning_rate=lr,\n",
    "                weight_decay=hparams[\"weight_decay\"]\n",
    "            )\n",
    "            model = compile_model(model, loss=loss, optimizer=optimizer)\n",
    "\n",
    "        earlystop_cb = EarlyStopping(\n",
    "            monitor='val_f1_score',\n",
    "            patience=3,\n",
    "        )\n",
    "        callbacks = [earlystop_cb]\n",
    "\n",
    "        history = model.fit(\n",
    "            sub_train_dataset.repeat(),\n",
    "            validation_data=val_dataset.repeat(),\n",
    "            initial_epoch=warmup_epoch,\n",
    "            epochs=total_epoch,\n",
    "            steps_per_epoch=optuna_steps,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weights\n",
    "        )\n",
    "\n",
    "        # Compute penalized F1 score\n",
    "        score, best_f1, best_prec, best_rec = penalized_f1_score(\n",
    "            history,\n",
    "            config=PENALIZED_F1_CONFIG,\n",
    "            mode='mean',\n",
    "            loss=True\n",
    "        )\n",
    "\n",
    "        # Log final losses for diagnostics\n",
    "        last_train_loss = history.history[\"loss\"][-1]\n",
    "        last_val_loss = history.history[\"val_loss\"][-1]\n",
    "\n",
    "        trial.set_user_attr(\"phase2_score\", float(score))\n",
    "        print(\n",
    "            f\"Penalized F1: {score:.4f}, \"\n",
    "            f\"Best F1: {best_f1:.4f}, \"\n",
    "            f\"P: {best_prec:.4f}, \"\n",
    "            f\"R: {best_rec:.4f}\"\n",
    "        )\n",
    "        print(f\"Last Epoch --> Train Loss: {last_train_loss} | Val Loss: {last_val_loss}\")\n",
    "\n",
    "        return score\n",
    "\n",
    "    finally:\n",
    "        # Ensure full cleanup between trials\n",
    "        cleanup(model, history, callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28f77f3-9d4a-49d8-a832-b70f62528be8",
   "metadata": {},
   "source": [
    "### 4.4 Optuna Study Creation and Execution\n",
    "\n",
    "An Optuna study is created using **SQLite-based persistence**, allowing trials to be resumed safely across sessions.  \n",
    "The study maximizes the penalized $F_1$ score over a fixed number of trials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95246df8-e8db-4e3b-9fde-1486d330c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persistent storage for Optuna study\n",
    "storage_dir = \"sqlite:///phase2_optimization/diseases-phase2_optimization.db\"\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    storage=storage_dir,\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "# Launch optimization process\n",
    "study.optimize(\n",
    "    lambda trial: objective_optimization(trial, config=OPTIMIZATION_CONFIG),\n",
    "    n_trials=OPTIMIZATION_CONFIG['opt_trials'],\n",
    "    gc_after_trial=True\n",
    ")\n",
    "\n",
    "# Extract best trial information\n",
    "best_trial = study.best_trial\n",
    "best_hparams = best_trial.user_attrs.get(\"hparams\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2abdacb-7186-4d9f-9694-35ac557c4806",
   "metadata": {},
   "source": [
    "### 4.5 Saving Phase 2 Optimization Metadata\n",
    "\n",
    "Final optimization results and experimental settings are serialized for reproducibility, reporting, and downstream training stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b8aa77e-d339-4480-84a2-37ec90dbd16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metadata for long-term storage\n",
    "metadata = { \n",
    "    \"best_trial_number\": int(best_trial.number), \n",
    "    \"best_value\": float(best_trial.value), \n",
    "    \"best_hparams\": best_hparams, \n",
    "    \"phase2_settings\": { \n",
    "        \"loss\": \"CategoricalCrossentropy\", \n",
    "        \"optimizer\": \"AdamW\", \n",
    "        \"lr_schedules\": \"CosineDecay\", \n",
    "        \"warmup_epochs\": OPTIMIZATION_CONFIG[\"opt_warmup_epoch\"],\n",
    "        \"unfreeze_epochs\": OPTIMIZATION_CONFIG[\"opt_unfreeze_epoch\"],\n",
    "        \"unfreeze_layer\": OPTIMIZATION_CONFIG[\"unfreeze_layer\"],\n",
    "        \"trials\": OPTIMIZATION_CONFIG[\"opt_trials\"]\n",
    "    }, \n",
    "    \"timestamp\": time.time(), \n",
    "    \"seed\": SEED\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0d806fe-ccab-4358-86fc-af8f6592e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist metadata to disk\n",
    "metadata_dir = \"./phase2_optimization/diseases-best_hparams.json\"\n",
    "with open(metadata_dir, \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1ba670-ce6d-44be-8a85-36a2475eb87d",
   "metadata": {},
   "source": [
    "## **Conclusion & Next Steps**\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "In this notebook, we successfully conducted **Phase 2: Optimization Hyperparameters Search** for a DenseNet-based head classifier using **Optuna**.  \n",
    "\n",
    "Key outcomes include:\n",
    "\n",
    "- Efficient exploration of **Optimization hyperparameters** (two stages learning rates, weight decay and etc.). \n",
    "- Implementation of a **DenseNet121 backbone** frozen with pre-trained weights in stage 1, then unfrozen from `conv5_block1_0_bn` layer in stage 2.  \n",
    "- Use of **ROI-based preprocessing** to focus on lung regions while preserving DenseNet’s expected input format.  \n",
    "- Creation of **training and validation Subset of datasets** optimized for Optuna trials, with class re-mapping and also search for best `class weight power`.  \n",
    "- Definition of a **custom objective function** using **penalized F1 score** to account for class imbalance and stabilize metric evaluation with loss distance penalized and also gap between `Precision` and `Recall`.  \n",
    "- Establishment of a **persistent Optuna study** for reproducibility and easy continuation of hyperparameter search.  \n",
    "\n",
    "This notebook lays the foundation for **reliable, reproducible, and professional hyperparameter optimization**, which is essential in medical AI applications where **accuracy, precision, and recall** are critical.\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps**\n",
    "\n",
    "The next phase of this project will focus on:\n",
    "\n",
    "1. **Fine-tuning & Evaluation**  \n",
    "   - Conduct full model training with **frozen and partially unfrozen DenseNet layers**.  \n",
    "   - Evaluate on **independent test sets** to ensure generalization.   \n",
    "2. **Developing Step**\n",
    "    - Gather all models and go one use these models in Real World!\n",
    "\n",
    "Now we ready for **Training DenseNet121 diseases classification Model**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26134f27-194f-4abc-9a9a-27384c46e9d7",
   "metadata": {},
   "source": [
    "## **Resources & References**\n",
    "\n",
    "#### Official Documentation\n",
    "\n",
    "**TensorFlow & DenseNet**\n",
    "\n",
    "- [TensorFlow main site](https://www.tensorflow.org) — comprehensive docs, tutorials, API guides  \n",
    "- [DenseNet121 in TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/applications/DenseNet121) — TF Keras application reference  \n",
    "- [Keras Applications — DenseNet](https://keras.io/api/applications/densenet/) — overview and usage examples  \n",
    "\n",
    "**Optuna (Hyperparameter Optimization)**\n",
    "\n",
    "- [Optuna Official Site](https://optuna.org) — documentation, examples, guides  \n",
    "- [Optuna Tutorials & API Docs](https://optuna.github.io/optuna-web-dev/) — how to define objectives, create studies, and use pruners  \n",
    "- [Optuna GitHub Repository](https://github.com/optuna/optuna) — source code, issues, and examples  \n",
    "\n",
    "---\n",
    "\n",
    "#### Medical Imaging with DenseNet (Selected Papers)\n",
    "\n",
    "- [A comparative study of CNNs for COVID‑19 detection on chest X‑rays](https://link.springer.com/article/10.1186/s13634-021-00755-1) — DenseNet121 performance vs other backbones  \n",
    "- [Evaluation of CNNs for COVID‑19 classification on chest X‑rays](https://arxiv.org/abs/2109.02415) — DenseNet and other CNNs evaluated for COVID‑19  \n",
    "- [Automated pneumonia detection using DenseNet121 and other CNNs](https://academic.oup.com/bjr/article-abstract/doi/10.1259/bjr.20201263/7476744) — DenseNet121 effectiveness for pneumonia vs normal classification  \n",
    "\n",
    "---\n",
    "\n",
    "#### Additional Reading\n",
    "\n",
    "- [Optuna Framework Paper (KDD 2019)](https://optuna.github.io/optuna-web-dev/) — Takuya Akiba et al., formal research paper describing Optuna  \n",
    "\n",
    "---\n",
    "\n",
    "*These resources provide both practical implementation guidance and scientific context for DenseNet-based lung disease classification and Optuna hyperparameter optimization.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9016c3a-bf1c-4780-b5c1-393a78e7fccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
