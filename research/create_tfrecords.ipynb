{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c694472f-1a52-4fbc-a1c2-986dd5a5b614",
   "metadata": {},
   "source": [
    "\n",
    "# Data Transformation and TFRecord Generation\n",
    "\n",
    "This notebook is the critical data conversion step, transforming the consolidated file path indices (from `all_image_mask_pairs.csv`) into the highly efficient **TFRecord** binary format. This format is native to TensorFlow and enables optimized, large-scale data loading, improved I/O performance, and seamless distribution across multiple workers or devices during model training.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Functions and Methodology\n",
    "\n",
    "## 1. Data Serialization and Sharding\n",
    "The main function, `create_tfrecord`, reads the master CSV index and the multi-class label mapping. It performs the following steps:\n",
    "* **Shuffling:** The dataset is globally shuffled before sharding to ensure class balance across the output files.\n",
    "* **Byte Conversion:** It iterates through each record, reads the raw image data (and mask data, if present) directly into byte strings, and converts the class name into its corresponding integer index.\n",
    "* **Sharding:** The entire dataset is partitioned into multiple (`num_shards=10` by default) distinct `.tfrecord` files. This strategy facilitates parallel data ingestion, which is essential for reducing I/O bottlenecks and improving GPU utilization during training.\n",
    "\n",
    "## 2. Robust Data Parsing (`_parse_function`)\n",
    "As a prototype and checking step to see that tfrecords are created successfully, a key component is the `_parse_function`, which defines how the serialized data should be read back into memory by the `tf.data` pipeline. This function ensures the pipeline is robust and versatile for both segmentation and classification models:\n",
    "* **Feature Mapping:** It maps the raw byte strings and integers back to the original data structure (`image`, `mask`, `class`).\n",
    "* **Handling Missing Masks:** It implements a **conditional logic (`tf.cond`)** to detect records where the segmentation mask was absent (encoded as an empty byte string). In this case, instead of failing, it substitutes a dummy tensor of zeros for the mask, allowing models (like classification models) that don't need the mask to still process the image record seamlessly.\n",
    "\n",
    "This notebook successfully converts the entire dataset into a high-performance, sharded format ready for the segmentation and classification training tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5cfba01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 18:48:10.924239: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-11 18:48:11.802126: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-11 18:48:14.665364: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from collections import Counter\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12637f92-528a-48e1-9a95-dca4481278b2",
   "metadata": {},
   "source": [
    "## Section 1: TFRecord Serialization and Sharding Logic\n",
    "\n",
    "This section defines all the necessary helper functions and the primary function, `create_tfrecord`, required to serialize the raw image data and corresponding labels into the **TFRecord** binary format. This serialization process involves converting file paths into raw bytes and class names into integer features, and then packaging them into sharded files for optimized dataset loading in TensorFlow.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1 TF Feature Encoding Utilities\n",
    "\n",
    "This group of helper functions prepares individual data elements (images, masks, and labels) for serialization into the `tf.train.Example` protocol buffer format.\n",
    "\n",
    "* **`_bytes_feature(value)`**: Encodes raw byte strings (used for images and masks) as a `tf.train.Feature`. It includes a check to automatically convert TensorFlow `constant` objects to NumPy format if necessary.\n",
    "* **`_int64_feature(value)`**: Encodes scalar integer values (used for the class label) as a `tf.train.Feature`.\n",
    "* **`image_to_bytes(path)`**: A simple file utility that opens an image file in binary read mode (`'rb'`) and returns its entire content as a raw byte string. This is crucial for embedding the image data directly into the TFRecord file, eliminating subsequent disk lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0f13192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    '''\n",
    "    Converts a value (bytes or Tensor) to a tf.train.Feature containing a bytes_list.\n",
    "    This is used for serializing raw image and mask data.\n",
    "    '''\n",
    "    # Check if the value is a TensorFlow constant, and if so, convert it to a NumPy array (raw bytes)\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()\n",
    "    # Create and return the TF Feature object\n",
    "    return tf.train.Feature(bytes_list= tf.train.BytesList(value= [value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    '''\n",
    "    Converts a scalar integer value to a tf.train.Feature containing an int64_list.\n",
    "    This is used for serializing class labels.\n",
    "    '''\n",
    "    # Create and return the TF Feature object\n",
    "    return tf.train.Feature(int64_list= tf.train.Int64List(value= [value]))\n",
    "\n",
    "def image_to_bytes(path):\n",
    "    '''\n",
    "    Reads the content of a file path (typically an image or mask) into raw bytes.\n",
    "    '''\n",
    "    # Open the file in binary read ('rb') mode\n",
    "    with open(path, 'rb') as f:\n",
    "        # Read the entire file content\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1717722-e27b-4f7a-810d-ef578b08bba6",
   "metadata": {},
   "source": [
    "### 1.2 `create_tfrecord` Function (Sharding and Writing)\n",
    "\n",
    "This is the core function that orchestrates the entire TFRecord creation process, taking the master CSV index and converting it into multiple sharded binary files.\n",
    "\n",
    "#### Execution Steps:\n",
    "1.  **Load and Shuffle:** It reads the `all_image_mask_pairs.csv` using pandas, then performs a global **random shuffle** (`df.sample(frac= 1)`) on the entire dataset to ensure that the resulting shards are class-balanced.\n",
    "2.  **Sharding Calculation:** It calculates the `shard_size` by dividing the total number of records by the requested `num_shards` (defaulting to 10) and using `math.ceil` to ensure all records are included.\n",
    "3.  **Iteration and Serialization:** The function loops through the defined number of shards. For each shard:\n",
    "    * It slices the shuffled DataFrame (`shard_df`) based on `start_idx` and `end_idx`.\n",
    "    * It initializes a `tf.io.TFRecordWriter` for the shard's output path (`./data/tfrecords/data_XX.tfrecord`).\n",
    "    * It iterates over the rows in `shard_df`, converts the image and mask paths to byte strings, and maps the class name to its integer ID (`label_int`).\n",
    "    * **Mask Handling:** It explicitly checks if a `mask_path` is present (`pd.notna`). If not, an empty byte string (`b''`) is used for the mask feature, which will be handled during the parsing step.\n",
    "    * **Writing Example:** The features are bundled into a `tf.train.Example` and serialized before being written to the TFRecord file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f1b3145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecord(csv_path, output_dir, class_mapping, num_shards= 10):\n",
    "    '''\n",
    "    Converts the dataset index (from CSV) and raw image data into sharded TFRecord files.\n",
    "    \n",
    "    This function reads the image paths, loads the image/mask bytes, converts labels \n",
    "    to integers, and writes the serialized data to a specified number of TFRecord shards.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV index file containing image/mask paths and classes.\n",
    "        output_dir (str): Directory where the TFRecord shards will be saved.\n",
    "        class_mapping (dict): Dictionary mapping class names to integer labels.\n",
    "        num_shards (int, optional): The number of output TFRecord files to create. Defaults to 10.\n",
    "    '''\n",
    "    # Read the master CSV file containing all image/mask paths\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Globally shuffle the DataFrame randomly and reset the index for balanced sharding\n",
    "    df = df.sample(frac= 1).reset_index(drop= True)\n",
    "    \n",
    "    # Calculate the size of each shard (using ceiling to ensure all rows are included)\n",
    "    shard_size = math.ceil(len(df) / num_shards)\n",
    "    \n",
    "    # Iterate through the desired number of shards\n",
    "    for shard_id in range(num_shards):\n",
    "        # Determine the start and end indices for the current shard\n",
    "        start_idx = shard_id * shard_size\n",
    "        end_idx = min((shard_id + 1) * shard_size, len(df))\n",
    "        # Extract the subset of the DataFrame for the current shard\n",
    "        shard_df = df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Construct the output file path for the current shard (e.g., data_00.tfrecord)\n",
    "        tfrecord_path = f'{output_dir}/data_{shard_id:02d}.tfrecord'\n",
    "        \n",
    "        # Initialize the TFRecord writer\n",
    "        with tf.io.TFRecordWriter(tfrecord_path) as writer:\n",
    "            # Iterate over each row (image record) in the current shard\n",
    "            for _, row in shard_df.iterrows():\n",
    "                # Convert image path to raw bytes\n",
    "                image_bytes = image_to_bytes(row['image_path'])\n",
    "                \n",
    "                # Convert mask path to raw bytes, or set to empty bytes if mask is missing (NaN)\n",
    "                mask_bytes = image_to_bytes(row['mask_path']) if pd.notna(row['mask_path']) else b''\n",
    "                \n",
    "                # Look up the integer label using the class mapping\n",
    "                label_int = class_mapping[row['class']]\n",
    "                \n",
    "                # Assemble the features dictionary using the helper functions\n",
    "                feature = {\n",
    "                    'image': _bytes_feature(image_bytes),\n",
    "                    'mask': _bytes_feature(mask_bytes),\n",
    "                    'class': _int64_feature(label_int)\n",
    "                }\n",
    "                \n",
    "                # Create a tf.train.Example protocol buffer\n",
    "                example = tf.train.Example(features= tf.train.Features(feature= feature))\n",
    "                \n",
    "                # Serialize the Example and write it to the TFRecord file\n",
    "                writer.write(example.SerializeToString())\n",
    "                \n",
    "            # Print a confirmation message once the shard is saved\n",
    "            print(f\"Saved shard {shard_id+1}/{num_shards} to {tfrecord_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68863f8-2787-40d2-b8ea-95473562d268",
   "metadata": {},
   "source": [
    "### 1.3 Execution of TFRecord Generation\n",
    "\n",
    "This final block executes the `create_tfrecord` function, initiating the entire data transformation process. It defines the necessary input and output paths and loads the class mapping configuration required for serialization.\n",
    "\n",
    "* **Input Data:** The script specifies that the paths should be read from the master index file: `csv_path = './data/all_image_mask_pairs.csv'`.\n",
    "* **Output Directory:** The resulting sharded TFRecord files are directed to `output_dir = './data/tfrecords'`.\n",
    "* **Mapping Loading:** It loads the four-class index mapping (COVID, Normal, Viral Pneumonia, Lung Opacity) from the `class_mapping.json` file created in the previous notebook.\n",
    "* **Function Call:** The `create_tfrecord` function is invoked with the specified paths, the loaded mapping, and the sharding parameter `num_shards=10`. The resulting output confirms the successful creation of all 10 sharded TFRecord files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "922e56e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved shard 1/10 to ./data/tfrecords/data_00.tfrecord\n",
      "Saved shard 2/10 to ./data/tfrecords/data_01.tfrecord\n",
      "Saved shard 3/10 to ./data/tfrecords/data_02.tfrecord\n",
      "Saved shard 4/10 to ./data/tfrecords/data_03.tfrecord\n",
      "Saved shard 5/10 to ./data/tfrecords/data_04.tfrecord\n",
      "Saved shard 6/10 to ./data/tfrecords/data_05.tfrecord\n",
      "Saved shard 7/10 to ./data/tfrecords/data_06.tfrecord\n",
      "Saved shard 8/10 to ./data/tfrecords/data_07.tfrecord\n",
      "Saved shard 9/10 to ./data/tfrecords/data_08.tfrecord\n",
      "Saved shard 10/10 to ./data/tfrecords/data_09.tfrecord\n"
     ]
    }
   ],
   "source": [
    "# --- TFRecord Generation Execution ---\n",
    "\n",
    "# Define the input path for the master index CSV file\n",
    "csv_path = './data/all_image_mask_pairs.csv'\n",
    "# Define the output directory where the sharded TFRecord files will be saved\n",
    "output_dir = './data/tfrecords'\n",
    "\n",
    "# Load the class-to-index mapping from the JSON file created earlier\n",
    "with open('./data/class_mapping.json', 'r') as f:\n",
    "    class_mapping = json.load(f)\n",
    "    \n",
    "# Execute the main function to read the CSV, serialize the data, and write 10 sharded TFRecord files\n",
    "create_tfrecord(csv_path, output_dir, class_mapping, num_shards= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617ef53d-86a5-4661-864a-75fd370b9796",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: TFRecord Validation and Data Parsing\n",
    "\n",
    "This section is dedicated to validating the structural integrity and balanced distribution of the newly created TFRecord files. It defines the crucial deserialization logic required for the `tf.data` pipeline and performs checks to ensure data is correctly parsed and that the earlier global shuffling resulted in balanced class distribution across the shards.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 Data Parsing Function (`_parse_function`) and Structural Check\n",
    "\n",
    "The `_parse_function` is the fundamental component for reading the binary TFRecord files into usable tensors within the TensorFlow environment.\n",
    "\n",
    "* **Deserialization:** It defines the expected schema of the serialized data using `tf.io.FixedLenFeature` (`image` as string/bytes, `mask` as string/bytes, and `class` as `int64`) and uses `tf.io.parse_single_example` to extract the raw features.\n",
    "* **Image Decoding:** The raw byte features for the image are decoded into a 3-channel tensor using `tf.image.decode_png(..., channels=3)`.\n",
    "* **Critical Mask Handling:** It implements a robust mechanism using **`tf.cond`** to handle records where a segmentation mask was not present (encoded as an empty byte string).\n",
    "    * If the mask byte string length is zero, it returns a tensor of zeros (`tf.zeros_like`) to act as a dummy mask.\n",
    "    * Otherwise, it decodes the mask into a 1-channel tensor (`tf.image.decode_png(..., channels=1)`).\n",
    "---\n",
    "\n",
    "#### Structural Verification\n",
    "A test is performed on the first shard (`data_00.tfrecord`) to confirm the function correctly returns the expected tensor shapes and integer labels.\n",
    "\n",
    "| Element | Example Output Shape / Value | Comment |\n",
    "| :--- | :--- | :--- |\n",
    "| **Image** | `(299, 299, 3)` | Image size and 3 color channels |\n",
    "| **Mask** | `(256, 256, 1)` | Mask size and 1 channel |\n",
    "| **Label** | `1`, `3`, `0`, etc. | Integer class index confirmed |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51546d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(proto):\n",
    "    '''\n",
    "    Deserializes a single serialized tf.train.Example (protocol buffer) \n",
    "    from a TFRecord file back into image, mask, and label tensors.\n",
    "\n",
    "    This function is the core of the tf.data input pipeline.\n",
    "    '''\n",
    "    # Define the expected features (schema) serialized in the TFRecord file\n",
    "    feature_description = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string), # Raw image bytes\n",
    "        'mask': tf.io.FixedLenFeature([], tf.string),  # Raw mask bytes (can be empty)\n",
    "        'class': tf.io.FixedLenFeature([], tf.int64),  # Integer class label\n",
    "    }\n",
    "    \n",
    "    # Parse the single serialized example using the defined schema\n",
    "    parsed_features = tf.io.parse_single_example(proto, feature_description)\n",
    "    \n",
    "    # Decode the raw image bytes (PNG format) into a 3-channel tensor\n",
    "    image = tf.image.decode_png(parsed_features['image'], channels=3)\n",
    "    \n",
    "    # Extract the raw mask bytes\n",
    "    mask_raw = parsed_features['mask']\n",
    "    \n",
    "    # Conditional logic (tf.cond) to handle records with missing segmentation masks\n",
    "    # Check if the mask byte string length is zero (b'')\n",
    "    mask = tf.cond(\n",
    "        tf.equal(tf.strings.length(mask_raw), 0),\n",
    "        # True branch: Create a dummy mask of zeros with the same height/width as the image, but 1 channel\n",
    "        lambda: tf.zeros_like(image[..., :1]),  # dummy mask with 1 channel zeros\n",
    "        # False branch: Decode the raw mask bytes (PNG format) into a 1-channel tensor\n",
    "        lambda: tf.image.decode_png(mask_raw, channels=1)\n",
    "    )\n",
    "    \n",
    "    # Extract the integer class label\n",
    "    label = parsed_features['class']\n",
    "    \n",
    "    # Return the processed tensors\n",
    "    return image, mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0a5f912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299, 299, 3) (256, 256, 1) 1\n",
      "(299, 299, 3) (256, 256, 1) 3\n",
      "(299, 299, 3) (256, 256, 1) 1\n",
      "(299, 299, 3) (256, 256, 1) 0\n",
      "(299, 299, 3) (256, 256, 1) 3\n"
     ]
    }
   ],
   "source": [
    "# --- TFRecord Validation: Structural Check ---\n",
    "\n",
    "# Load the first TFRecord shard into a tf.data.Dataset\n",
    "dataset1 = tf.data.TFRecordDataset(['./data/tfrecords/data_00.tfrecord'])\n",
    "\n",
    "# Apply the parsing function to map the raw serialized data into tensors (image, mask, label)\n",
    "dataset1 = dataset1.map(_parse_function)\n",
    "\n",
    "# Iterate through the first 5 records to verify successful parsing and tensor shapes\n",
    "for (img, mask, label) in dataset1.take(5):\n",
    "    # Print the shape of the image and mask tensors, and the numpy value of the label\n",
    "    print(img.shape, mask.shape, label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e059bc32-09a5-4662-b09b-0be92356a481",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.2 Randomness and Class Distribution Check\n",
    "\n",
    "To validate that the global shuffling before sharding was successful, a check is performed on the class balance of a single, randomly selected shard (in this case, the last one, `data_09.tfrecord`).\n",
    "\n",
    "* **`class_counts_stream` Function:** This helper function iterates through the dataset and uses Python's `collections.Counter` to tally the occurrence of each class index.\n",
    "* **Result:** The class distribution check on `data_09.tfrecord` shows a mixed set of class indices, confirming that the data was successfully shuffled and distributed across the sharded files, which is necessary for efficient and balanced distributed training.\n",
    "\n",
    "$$\\text{Counts} = \\{\\text{1: 1061, 3: 583, 0: 354, 2: 114}\\}$$\n",
    "(Where 1=**Normal**, 3=**Lung\\_Opacity**, 0=**COVID**, 2=**Viral Pneumonia**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d05db2-86a3-4905-8dda-1ea101ac3d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_counts_stream(dataset):\n",
    "    '''\n",
    "    Iterates through a dataset and calculates the count of each unique class label.\n",
    "    This is used to verify the effectiveness of the global shuffling before sharding.\n",
    "    '''\n",
    "    # Initialize a Counter object\n",
    "    cnt = Counter()\n",
    "    # Iterate through the dataset, ignoring the image and mask tensors\n",
    "    for _, _, label in dataset:\n",
    "        # label is a tf.Tensor scalar -> convert to Python int\n",
    "        # Convert the TensorFlow scalar Tensor to a Python integer before incrementing the count\n",
    "        cnt[int(label.numpy())] += 1\n",
    "    # Return the Counter object containing the class counts\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e89432de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 1061, 3: 583, 0: 354, 2: 114})\n"
     ]
    }
   ],
   "source": [
    "dataset2 = tf.data.TFRecordDataset(['./data/tfrecords/data_09.tfrecord'])\n",
    "dataset2 = dataset1.map(_parse_function)\n",
    "\n",
    "counts = class_counts_stream(dataset2)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307a588d-67e0-4b58-848a-5e39fb238771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
